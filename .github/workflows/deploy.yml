name: Deploy

on:
  push:
    branches: [main]
  workflow_dispatch:  # Allow manual trigger

env:
  VPS_HOST: 170.64.137.4
  VPS_USER: root
  APP_DIR: /opt/apps/goalix

jobs:
  preflight:
    name: Pre-flight Checks
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Verify SSH connectivity
        uses: appleboy/ssh-action@v1.0.3
        with:
          host: ${{ env.VPS_HOST }}
          username: ${{ env.VPS_USER }}
          key: ${{ secrets.VPS_SSH_KEY }}
          script: |
            echo "SSH connection successful"
            echo "Host: $(hostname)"
            echo "Date: $(date)"

      - name: Validate production environment
        uses: appleboy/ssh-action@v1.0.3
        with:
          host: ${{ env.VPS_HOST }}
          username: ${{ env.VPS_USER }}
          key: ${{ secrets.VPS_SSH_KEY }}
          script: |
            set -e
            echo "=== Checking production environment ==="

            # Check app directory exists
            if [ ! -d "${{ env.APP_DIR }}" ]; then
              echo "ERROR: App directory ${{ env.APP_DIR }} does not exist"
              exit 1
            fi

            # Check .env file exists
            if [ ! -f "${{ env.APP_DIR }}/.env" ]; then
              echo "ERROR: .env file not found at ${{ env.APP_DIR }}/.env"
              echo "Please create the .env file with production credentials"
              exit 1
            fi

            # Validate required environment variables in .env
            cd ${{ env.APP_DIR }}
            REQUIRED_VARS="DATABASE_URL REDIS_URL NEXTAUTH_SECRET NEXTAUTH_URL"
            MISSING_VARS=""

            for VAR in $REQUIRED_VARS; do
              if ! grep -q "^${VAR}=" .env; then
                MISSING_VARS="$MISSING_VARS $VAR"
              fi
            done

            if [ -n "$MISSING_VARS" ]; then
              echo "ERROR: Missing required environment variables in .env:$MISSING_VARS"
              exit 1
            fi

            # Check for localhost in production URLs (common mistake)
            if grep -E "^(DATABASE_URL|REDIS_URL|NEXTAUTH_URL)=.*localhost" .env; then
              echo "ERROR: Found 'localhost' in production URLs - this will fail in Docker"
              exit 1
            fi

            echo "=== All pre-flight checks passed ==="

  deploy:
    name: Deploy to Production
    runs-on: ubuntu-latest
    needs: preflight

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Deploy to VPS
        uses: appleboy/ssh-action@v1.0.3
        with:
          host: ${{ env.VPS_HOST }}
          username: ${{ env.VPS_USER }}
          key: ${{ secrets.VPS_SSH_KEY }}
          script: |
            set -e

            echo "=== Deploying Goalix ==="
            cd ${{ env.APP_DIR }}

            # Pull latest changes
            echo "Pulling latest code..."
            git fetch origin main
            git reset --hard origin/main

            # CAPA: Robust container cleanup to prevent stale state issues
            echo "Cleaning up containers (robust)..."

            # 1. Stop and remove by name (catches running containers)
            docker stop goalix goalix-redis 2>/dev/null || true
            docker rm -f goalix goalix-redis 2>/dev/null || true

            # 2. Remove any containers with goalix in the name (catches renamed/orphaned)
            docker ps -a --filter "name=goalix" -q | xargs -r docker rm -f 2>/dev/null || true

            # 3. Docker compose down with all cleanup flags
            docker compose down --volumes=false --remove-orphans --timeout 30 2>/dev/null || true

            # 4. Remove any dangling containers from this project
            docker container prune -f --filter "label=com.docker.compose.project=goalix" 2>/dev/null || true

            # Build and restart containers with force-recreate
            echo "Building and restarting containers..."
            docker compose build --no-cache
            docker compose up -d --force-recreate --remove-orphans

            # Note: For schema changes, run migrations manually:
            # docker run --rm --network n8n-docker-caddy_n8n_network \
            #   -v /opt/apps/goalix/prisma:/app/prisma -w /app \
            #   -e DATABASE_URL="$DATABASE_URL" \
            #   node:22-alpine sh -c "npm i prisma@6.19.1 && npx prisma migrate deploy"

            # Health check with retry (using docker exec since port is only exposed to containers)
            echo "Waiting for app to start..."
            for i in 1 2 3 4 5; do
              sleep 10
              echo "Health check attempt $i..."
              if docker compose exec -T app wget -q --spider http://127.0.0.1:3000/api/health 2>/dev/null; then
                echo "App is healthy!"
                break
              fi
              if [ $i -eq 5 ]; then
                echo "Health check failed after 5 attempts"
                docker compose logs app
                exit 1
              fi
            done

            echo "=== Deployment complete ==="

      - name: Notify on failure
        if: failure()
        run: |
          echo "Deployment failed! Check the logs above."
          # Add Slack/Discord notification here if needed

  verify:
    name: Verify Deployment
    runs-on: ubuntu-latest
    needs: deploy

    steps:
      - name: Wait for deployment to stabilize
        run: sleep 15

      - name: Health check
        run: |
          response=$(curl -s -o /dev/null -w "%{http_code}" https://goalzenix.com/api/health)
          if [ "$response" != "200" ]; then
            echo "Health check failed with status: $response"
            exit 1
          fi
          echo "Health check passed!"

      - name: Smoke test - Landing page
        run: |
          response=$(curl -s -o /dev/null -w "%{http_code}" https://goalzenix.com)
          if [ "$response" != "200" ]; then
            echo "Landing page check failed with status: $response"
            exit 1
          fi
          echo "Landing page check passed!"
